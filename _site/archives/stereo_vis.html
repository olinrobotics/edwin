<!DOCTYPE html>
<html lang="en">
  
  <head>
  <meta charset="UTF-8">
  <title>Human Interactions Robotics Laboratory</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css">
  <link rel="stylesheet" href="/irl/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/irl/css/cayman.css">
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- Bootstrap core CSS -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet">
  <!-- Material Design Bootstrap -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.5.11/css/mdb.min.css" rel="stylesheet">
</head>

  <body>
    <section class="page-header">
    <h1 class="project-name">Human Interactions Robotics Laboratory</h1>
    <h2 class="project-tagline">HIRo is working to build a low-cost robotic coworker that can assist in workplace tasks and interact with other humans around it.</h2>
    <a href="/irl/index.html" class="btnCay">About</a>
    <a href="/irl/projects.html" class="btnCay">Our Projects</a>
    <a href="https://github.com/olinrobotics/irl/wiki" rel="noopener" target="_blank" class="btnCay">Tutorials</a>
    <a href="https://github.com/olinrobotics/irl/" rel="noopener" target="_blank" class="btnCay">View on GitHub</a>
</section>

    <section class="main-content">
      
      <h1 id="stereoscopic-vision">Stereoscopic Vision</h1>

<h3 id="overview">Overview</h3>

<p>The goal of this research was to intuitively teach Edwin to recognize and remember objects, thus creating a robust tool for dataset generation. This decreased the amount of time we would need to spend generating datasets of individual objects for any future machine learning.</p>

<p>In order to identify objects we were interested in “teaching” to Edwin, we developed a stereoscopic camera system that could locate an objects distance from Edwin. We then tagged any objects closer than a certain distance as “significant.”</p>

<h3 id="the-hardware">The Hardware</h3>

<p>Starting this project, we weren’t sure what features our prospective cameras would need.
  We ultimately chose the Hardkernal oCam both because it was a relatively high quality camera (which could be focused to different distances) and because we had a pair of oCams from a previous project (allowing us to start experimenting  with vision right away.)</p>

<h4 id="our-cameras">Our Cameras</h4>

<p><img src="/irl/assets/images/archive/ocam.jpg" alt="ocam" /></p>

<p>Hardkernel oCam</p>

<ul>
  <li>standard M12 lens</li>
  <li>3.6mm focal length</li>
  <li>65 deg. field of view (FOV)</li>
  <li>120 frames per second (fps) at a resolution of 640x480 pixels</li>
  <li>5 megapixels (MP)</li>
  <li>Micro-USB 3.0 connection port</li>
  <li>35 grams</li>
  <li>$99 USD + shipping</li>
</ul>

<h4 id="the-camera-mount">The camera mount</h4>

<p>We originally planned to hold the oCams steady by CADing and 3D printing two mounts to hold them snugly. We then fixed the mounts in place relative to each other by bolting them to a piece of sheet metal cut to size.</p>

<p><img src="/irl/assets/images/archive/oCam_Mount.png" alt="oCam_Mount" /></p>

<p>The bolts weren’t sufficient to keep the mounts from shifting minutely, though, and this resulted in offsets in camera calibration. In the end, we merged the two camera mounts into one unified, 3D printed mount.</p>

<p><em>Insert CAD model</em></p>

<p>This was a better solution, as the single mount was structurally stiffer than two constrained single camera mounts.</p>

<p><em>Insert transparent assembly render</em></p>

<p>The mount was held in place inside the head through 4 screw “pins,” (2 on either side of the head). The pin forces exerted by the screws were sufficient to hold the mount immobile in the head under stress.</p>

<h4 id="future-improvements">Future improvements</h4>

<p><strong>An additional clip on the back of the camera mount (to secure the cable to the mount)</strong></p>

<p>While our mount design was adequate for holding our cameras steady, the micro-USB 3.0 to USB 2.0 cables had a tendency to come unplugged from the cameras when handled or disturbed too much. We resoldered the micro-USB 3.0 female ports on the oCams, which reduced instances of this problem, but did not completely solve them.</p>

<p><strong>Better (and cheaper) cameras</strong></p>

<p>Having used the oCams for a semester, we have identified the important factors when choosing a camera for visual processing.
  Of these factors, the most relevant to us are:</p>

<ol>
  <li>The shutter (global v. rolling)</li>
  <li>Camera synchronization</li>
  <li>Field of view (Wide v. narrow)</li>
  <li>Color v. Black and White</li>
</ol>

<p>Global shutter: Since our cameras are mounted inside Edwin’s head, which moves fairly often as he executes behaviors, we need to be able to capture pixels in our images instantaneously, rather than sequentially. This will allow us to avoid image blurs and distortions from the motion of the cameras, which could potentially interfere with our visual processing.</p>

<p>Camera synchronization: If we could synchronize video feeds from both cameras, we would eliminate a potential source of error in our visual processing.</p>

<p>Wide FOV: If we can provide more data for our visual processing, we can cope with missing information more easily, thus creating a more accurate distance estimate overall.</p>

<p>Color: While most visual processing only requires black and white camera feeds, we also account for the dominant color of an object, which requires a color camera.</p>

<h4 id="arduino-powered-turntable">Arduino powered turntable</h4>

<h3 id="the-software">The Software</h3>

<h4 id="opencv">OpenCV</h4>

<h5 id="optical-flow">Optical Flow</h5>

<h5 id="color-detection">Color Detection</h5>

<p>#####</p>

<p><em>This page is under construction. Last edited on 1/29/17 by <a href="https://github.com/Oktober13">L. Zuehsow</a>.</em></p>


      <footer class="site-footer">
    <span class="site-footer-owner"><a href="/irl" rel="noopener" target="_blank" >Human Interactions Robotics Laboratory</a> is maintained by <a
            href="https://olinrobotics.github.io/" rel="noopener" target="_blank" >Olin Robotics</a>.</span>
    <span class="site-footer-credits">This page was generated by <a
            href="https://pages.github.com" rel="noopener" target="_blank" >GitHub Pages</a>.</span>
</footer>


    </section>

    <!-- JQuery -->
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <!-- Bootstrap tooltips -->
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.4/umd/popper.min.js"></script>
    <!-- Bootstrap core JavaScript -->
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js"></script>
    <!-- MDB core JavaScript -->
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.5.11/js/mdb.min.js"></script>
  </body>
</html>
